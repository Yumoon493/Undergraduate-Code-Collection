# -*- coding: utf-8 -*-
# æœ€ä½å†…å­˜è¦æ±‚ï¼š4GB RAM
# å®‰è£…å‘½ä»¤ï¼ˆåœ¨CMDä¸­æ‰§è¡Œï¼‰ï¼š
# pip install paddlepaddle==2.6.0 paddlenlp==2.6.0 numpy==1.26.0 psutil datasets

import os
import gc
import signal
import psutil
import paddle
import numpy as np
from functools import partial
from paddle.io import DataLoader
from paddlenlp.data import DataCollatorForTokenClassification
from paddlenlp.transformers import BertTokenizer, BertForTokenClassification
from paddlenlp.metrics import ChunkEvaluator

# ==== å†…å­˜ç®¡ç†åˆå§‹åŒ– ====
paddle.set_device('cpu')  # å¼ºåˆ¶CPUæ¨¡å¼
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # ç¦ç”¨tokenizerå¹¶è¡Œ
os.environ['FLAGS_allocator_strategy'] = 'auto_growth'  # åŠ¨æ€å†…å­˜åˆ†é…

# å†…å­˜ç›‘æ§é…ç½®
MAX_MEMORY_MB = 2500  # è®¾ç½®æœ€å¤§å…è®¸å†…å­˜ï¼ˆå•ä½MBï¼‰
process = psutil.Process(os.getpid())


def memory_safe():
    """å†…å­˜å®‰å…¨æ£€æŸ¥"""
    current_mem = process.memory_info().rss / 1024 ** 2
    if current_mem > MAX_MEMORY_MB * 0.8:
        print(f"âš ï¸ å†…å­˜å‘Šè­¦: {current_mem:.1f}MB > {MAX_MEMORY_MB * 0.8:.1f}MB")
        return False
    return True


# ==== æ•°æ®æµå¼åŠ è½½ ====
def load_streaming_data():
    from datasets import load_dataset
    return load_dataset(
        "json",
        data_files={
            "train": "data/processed/train.json",
            "dev": "data/processed/dev.json",
            "test": "data/processed/test.json"
        },
        streaming=True  # å¯ç”¨æµå¼åŠ è½½
    )


# ==== æ•°æ®å¤„ç†ç®¡é“ ====
class DataProcessor:
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        self.label2id = {"O": 0, "B": 1, "M": 2, "E": 3, "S": 4}
        self.collator = DataCollatorForTokenClassification(
            self.tokenizer,
            label_pad_token_id=self.label2id["O"],
            pad_to_multiple_of=32  # å†…å­˜å¯¹é½ä¼˜åŒ–
        )

    def process(self, example):
        """æµå¼å¤„ç†å•ä¸ªæ ·æœ¬"""
        if not memory_safe():
            raise MemoryError("å†…å­˜è¶…å‡ºå®‰å…¨é˜ˆå€¼")

        # æ–‡æœ¬å¤„ç†
        text = example["text"].strip().split(" ")
        label = example.get("label", "")

        # Tokenize
        inputs = self.tokenizer(
            text,
            max_length=64,  # è¿›ä¸€æ­¥ç¼©çŸ­é•¿åº¦
            truncation=True,
            is_split_into_words=True,
            return_length=True
        )

        # æ ‡ç­¾å¤„ç†
        if label:
            label_ids = [self.label2id.get(tag, 0) for tag in label.split()]
            label_ids = label_ids[:64 - 2]  # æˆªæ–­
            inputs["labels"] = [0] + label_ids + [0]
            assert len(inputs["labels"]) == len(inputs["input_ids"])

        return inputs


# ==== æ¨¡å‹å®šä¹‰ ====
def load_model():
    model = BertForTokenClassification.from_pretrained(
        'bert-base-chinese',
        num_classes=5,
        ignore_mismatched_sizes=True
    )
    model.eval()  # åˆå§‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼èŠ‚çœå†…å­˜
    return model


# ==== è®­ç»ƒæµç¨‹ ====
class SafeTrainer:
    def __init__(self):
        self.batch_size = 2  # æ›´å°çš„æ‰¹æ¬¡
        self.num_epochs = 1  # åˆå§‹è®¾ä¸º1ä¸ªepochè°ƒè¯•

    def create_streaming_loader(self, dataset):
        return DataLoader(
            dataset=dataset.map(self.processor.process, batched=False),
            batch_size=self.batch_size,
            collate_fn=self.processor.collator,
            num_workers=0  # å¿…é¡»è®¾ä¸º0
        )

    def train(self):
        try:
            # åˆå§‹åŒ–ç»„ä»¶
            self.processor = DataProcessor()
            model = load_model()
            optimizer = paddle.optimizer.AdamW(
                learning_rate=3e-5,
                parameters=model.parameters()
            )

            # æµå¼æ•°æ®åŠ è½½
            dataset = load_streaming_data()
            train_loader = self.create_streaming_loader(dataset["train"])

            # ç²¾ç®€è®­ç»ƒå¾ªç¯
            model.train()
            for epoch in range(self.num_epochs):
                print(f"==== Epoch {epoch + 1} =====")

                for step, batch in enumerate(train_loader):
                    if not memory_safe():
                        raise MemoryError("è®­ç»ƒç»ˆæ­¢ï¼šå†…å­˜è¶…é™")

                    # å‰å‘è®¡ç®—
                    outputs = model(**batch)
                    loss = outputs.loss

                    # åå‘ä¼ æ’­
                    loss.backward()
                    optimizer.step()
                    optimizer.clear_grad()

                    # å†…å­˜æ¸…ç†
                    del batch, outputs, loss
                    gc.collect()

                    if step % 10 == 0:
                        print(f"Step {step} | Mem: {process.memory_info().rss / 1024 ** 2:.1f}MB")

        except Exception as e:
            print(f"âŒ è®­ç»ƒå¼‚å¸¸: {str(e)}")
            print("å»ºè®®æ“ä½œï¼š")
            print("1. å…³é—­å…¶ä»–ç¨‹åºé‡Šæ”¾å†…å­˜")
            print("2. å°†batch_sizeè®¾ä¸º1")
            print("3. å‡å°‘max_lengthåˆ°32")


# ==== æ‰§è¡Œå…¥å£ ====
if __name__ == "__main__":
    # æ³¨å†Œä¸­æ–­ä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, lambda *_: (print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­"), exit()))

    # å¯åŠ¨è®­ç»ƒ
    print("=== å®‰å…¨è®­ç»ƒæ¨¡å¼å¯åŠ¨ ===")
    print(f"å½“å‰å†…å­˜: {process.memory_info().rss / 1024 ** 2:.1f}MB")

    try:
        trainer = SafeTrainer()
        trainer.train()
    except KeyboardInterrupt:
        print("æ­£å¸¸é€€å‡º")
    finally:
        print("æœ€ç»ˆå†…å­˜:", process.memory_info().rss / 1024 ** 2, "MB")